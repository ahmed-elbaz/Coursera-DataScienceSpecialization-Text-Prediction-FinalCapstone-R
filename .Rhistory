y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y~x)
dfbetas(fit)
library(swirl)
swirl()
library(swirl)
swirl()
?shuttle
MASS::shuttle
library(MASS)
data("shuttle")
?shuttle
head(shuttle)
str(shuttle)
fit <- glm(use ~ wind, family='binomial', shuttle)
fit$coefficients
data("InsectSprays")
head(InsectSprays)
fit4 <- glm(count ~ spray -1, poisson, InsectSprays)
coef(fit)
coef(fit4)
cof <- exp(coef(fit4))
cof
cof[1]
cof[1]/cof[2]
library(swirl)
swirl()
library(swirl)
swirl()
swirl()
0
exit()
bye()
swirl()
bye()
swirl()
rgp1()
rgp2()
head(swiss)
md1 <- lm(Fertility ~ ., data= swiss)
mdl <- lm(Fertility ~ ., data= swiss)
vif(mdl)
mdl <- lm(Fertility ~ Agriculture+Education+Catholic+Infant.Mortality, data= swiss)
mdl2 <- lm(Fertility ~ Agriculture+Education+Catholic+Infant.Mortality, data= swiss)
vif(mdl2)
0
bye()
swirl()
xlc <- simbias()
x1c <- simbias()
apply(x1c, 1, mean)
fit1 <- lm(Fertility, Agriculture, data= swiss)
fit1 <- lm(Fertility, Agriculture, data= swiss)
fit1 <- lm(Fertility~Agriculture, data= swiss)
fit3 <- lm(Fertility~Agriculture+Examination+Education, data= swiss)
anova(fit1, fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- deviance(fit1)-deviance(fit3)
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
ravenData
mdl <- (ravenWinNum ~ ravenScore, family=binomial, data= ravenData)
mdl <- glm(ravenWinNum ~ ravenScore, family=binomial, data= ravenData)
lodds <- predict(mdl,data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
confint(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(0.95, 1)
var(rpois(1000, 50))
nxt()
head(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
1
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits + 1))
qpois(.95, mdl2$fitted.values[704])
mtcars
?mtcars
class(mtcars$am)
sapply(mtcars,class)
library(ggplot)
library(ggplot2)
library(dplyr)
Motorcars <- mtcars
rm(Motorcars)
motorcars <- mtcars
head(motorcars)
sapply(motorcars,class)
motorcars$cyl <- as.factor(motorcars$cyl)
sapply(motorcars,class)
motorcars$vs <- as.factor(motorcars$vs)
motorcars$am <- as.factor(motorcars$am)
motorcars$gear <- as.factor(motorcars$gear)
boxplot(mpg~am)
boxplot(mpg~am, data=motorcars)
boxplot(mpg~am, data=motorcars, xlab = "Transmission(0=automatic,1=manual)", ylab = "Miles/(US) gallon", main = "mpg against transmission type")
g <- ggplot(motorcars, aes(x=am, y=mpg)) +  geom_boxplot(aes(color=am)) +  ggtitle("mpg against transmission type") + labs(x="Transmission(0=automatic,1=manual)",y="mpg")
g
g <- ggplot(motorcars, aes(x=am, y=mpg)) +  geom_boxplot(aes(fill=am)) +  ggtitle("mpg against transmission type") + labs(x="Transmission(0=automatic,1=manual)",y="mpg")
g
fit <- lm(mpg ~ ., data=motorcars)
fit
summary(fit)
vif(fit)
sqrt(vif(fit))
summary(fit)$coef
basicFit <- lm(mpg ~ am, mtcarsFactors)
basicFit <- lm(mpg ~ am, motorcars)
summary(basicFit)
summary(basicFit)$rsquared
summary(fit)
fullFit <- lm(mpg ~ ., data=motorcars)
summary(fullFit)
soloFit <- lm(mpg ~ am, motorcars)
summary(soloFit)
cor(mtcars)
round(cor(mtcars),3)
bestFit <- lm(mpg, cyl + disp + wt + am)
bestFit <- lm(mpg, cyl + disp + wt + am, data= motorcars)
bestFit <- lm(mpg ~ cyl + disp + wt + am, data= motorcars)
summary(bestFit)
bestFit <- lm(mpg ~ cyl + wt + am, data= motorcars)
summary(bestFit)
bestFit <- lm(mpg ~ cyl + disp + wt + am, data= motorcars)
summary(bestFit)
bestFit <- lm(mpg ~ cyl + disp + wt + am, data= motorcars)
summary(bestFit)
plot(bestFit)
plot(bestFit)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
head(segmentationOriginal)
library(dplyr)
table(segmentationOriginal$Case)
training <- segmentationOriginal[Case==Train,]
training <- segmentationOriginal[segmentationOriginal$Case==Train,]
training <- segmentationOriginal[segmentationOriginal$Case=="Train",]
dim(training)
testing <- segmentationOriginal[segmentationOriginal$Case=="Test",]
dim(testing)
set.seed(125)
modFit <- train( Class~., method="rpart",data=training)
install.packages('e1071', dependencies=TRUE)
modFit <- train( Class~., method="rpart",data=training)
modFit$finalModel
table(training$Class)
install.packages("rottle")
plot(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE)
text(modFit$finalModel,use.n = TRUE, all =TRUE)
text(modFit$finalModel,use.n = TRUE, all =TRUE, cex=0.8)
plot(modFit$finalModel, uniform = TRUE)
text(modFit$finalModel,use.n = TRUE, all =TRUE, cex=0.8)
modFit$finalModel
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
dim(olive)
head(olive)
install.packages("tree")
library(tree)
modTree <- tree(Area~., olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(modTree, newdata)
plot(modTree)
modTree
table(olive$Area)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
install.packages("ElemStatLearn")
install.packages("ElemStatLearn")
library(ElemStatLearn)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
dim(trainSA)
head(trainSA)
modSA <- train(chd~age+alcohol+obesity+tobacco+typea+ldl, data = trainSA, method="glm", family = "binomial")
class(trainSA$chd)
modSA
modSA$fullModel
modSA$finalModel
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(modSA,trainSA)
missClass(trainSA$chd,predict(modSA,trainSA))
missClass(testSA$chd,predict(modSA,testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
class(vowel.train$y)
Training <- vowel.train
Testing <- vowel.test
Training$y <- as.factor(Training$y)
class(Training$y)
Testing$y <- as.factor(Testing$y)
set.seed(33833)
modVowel <- train(y~., method = "rf", data = Training)
varImp(modVowel)
modVowel <- randomForest((y ~., data=vowel.train))
modVowel <- randomForest(y ~., data=vowel.train)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
training <- vowel.train
testing <- vowel.test
training$y <- as.factor(training$y)
class(training$y)
testing$y <- as.factor(testing$y)
class(testing$y)
set.seed(33833)
library(caret)
modeRF <- train(y~., method="rf", data = training)
modeBoost <- train(y~., method="gbm", data = training)
modeBoost <- train(y~., method="gbm", data = training)
pred1 <- predict(modeRF,testing)
pred2 <- predict(modeBoost,testing)
predComb <- data.frame(pred1, pred2, y = testing$y)
combModFit <- train(y~., method = "gam", data =predComb)
pred3 <- predict(combModFit, predComb)
sqrt(sum((pred1-testing$y)^2))
pred1$finalModel
confusionMatrix(pred1,testing$y)
confusionMatrix(pred2,testing$y)
confusionMatrix(pred3,testing$y)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
head(training)
modRF <- train(diagnosis~.,method="rf", data = training)
modBoost <- train(diagnosis~.,method="gbm", data = training)
modlda <- train(diagnosis~.,method="lda", data = training)
predRF <- predict(modRF,testing)
predBoost <- predict(modBoost,testing)
predlda <- predict(modlda, testing)
dataComb <- data.frame(predRF, predBoost, predlda, diagnosis = testing$diagnosis)
modComb <- train(diagnosis~., method="rf", data = dataComb, verbose = FALSE)
predComb<- predict(modComb,dataComb)
cfRF <- confusionMatrix(teting$diagnosis, predRF)
cfRF <- confusionMatrix(testing$diagnosis, predRF)
cfBoost <- confusionMatrix(testing$diagnosis, predBoost)
cflda <- confusionMatrix(testing$diagnosis, predlda)
cfcomd <- confusionMatrix(testing$diagnosis, predComb)
cfRF
cfRF[accuracy]
cfRF[1]
cfRF$overall
cfRF$overall[1]
cfBoost$overall[1]
cflda$overall[1]
cfcomd$overall[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
head(training)
modLasso <- train(CompressiveStrength~., method = "lasso", data = training)
modLasso <- train(CompressiveStrength~., method = "lasso", data = training)
?plot.enet
plot.enet(modLasso$finalModel,  xvar="penalty", use.color=TRUE)
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
getwd()
setwd("~/Desktop")
url = "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
library(lubridate) # For year() function below
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
head(training)
head(testing)
install.packages("forecast ")
library(forecast)
bats
install.packages("forecast")
library(forecast)
ts1 <- bats(training)
tstrain = ts(training$visitsTumblr)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
modSvm <- svm(CompressiveStrength ~ ., data = training)
predSvm <- predict(modSvm, testing)
accSvm <- accuracy(predSvm, testing$CompressiveStrength)
data.frame(accSvm)["RMSE"]
library(e1071)
set.seed(325)
modSvm <- svm(CompressiveStrength ~ ., data = training)
predSvm <- predict(modSvm, testing)
accSvm <- accuracy(predSvm, testing$CompressiveStrength)
data.frame(accSvm)["RMSE"]
accSvm
install.packages("plotly")
install.packages("plotly")
library(plotly)
plot_ly(mtcars, x =wt, y =mpg, mode="markers")
data(mtcars)
plot_ly(mtcars, x =wt, y =mpg, mode="markers")
head(mtcars)
plot_ly(mtcars, x = wt , y =mpg, mode="markers")
plot_ly(mtcars, x = cyl , y =mpg, mode="markers")
plot_ly(mtcars, x = mtcars$cyl , y =mpg, mode="markers")
plot_ly(mtcars, x = mtcars$cyl , y =mtcars$mpg, mode="markers")
head(mtcars)
plot_ly(mtcars, x = mtcars$wt , y =mtcars$mpg, mode="markers")
install.packages("leaflet")
library(leaflet)
my_map <- leaflet() %>% addTiles()
my_map
set.seed(2014-04-25)
df <- data.frame
df <- data.frame(lat = runif(20,min = 39.2, max =39.3), lng = runif(20,min = -76.6, max= -76.5))
df %>% leaflet() %>% addTiles() %>% addMarkers()
shiny::runApp('E:/Data science/JHU Coursera/Course 9 Developing Data Products/Week 4 course project/second try/Mathematics')
runApp('E:/Data science/JHU Coursera/Course 9 Developing Data Products/Week 4 course project/second try/Mathematics')
runApp('E:/Data science/JHU Coursera/Course 9 Developing Data Products/Week 4 course project/second try/Mathematics')
runApp('E:/Data science/JHU Coursera/Course 9 Developing Data Products/Week 4 course project/second try/Mathematics')
shiny::runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
UnigramsSorted <- readRDS("data/Unigrams.rds")
shiny::runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
shiny::runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
nextWordPred("Talking to your mom has the same effect as a hug and helps reduce your")
nextWordPred("When you were in Holland you were like 1 inch away from me but you hadn't time to take a")
nextWordPred("I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the")
nextWordPred("Every inch of you is perfect from the bottom to the
")
nextWordPred("Every inch of you is perfect from the bottom to the")
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
nextWordPred("the")
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
nextWordPred <- function(sentence){
# clean the input sentence
words <- tolower(sentence)
words <- gsub("[[:punct:]]","", words ) #remove punctuation
words <- gsub("\\d", "", words)  #reomve numbers
words <- str_trim(words) #remove unnecessary spaces
words <- unlist(strsplit(words, split = " ")) # split the sentences into words
if(length(words)>1){
words4gram <- paste(words[(length(words)-2):length(words)], collapse = " ") # select last 3 words
# Output the most frequent 4 gram predicted word
Predicted4gram <- stri_extract_last_words(FourgramsSorted[word %like% paste0("^",words4gram)][1]$word)
if(is.na(Predicted4gram) == FALSE){
return(Predicted4gram)
} else if(is.na(Predicted4gram) == TRUE) {
words3gram <- paste(words[(length(words)-1):length(words)], collapse = " ") # select last 2 words
# Output the most frequent Trigram predicted word
PredictedTrigram <- stri_extract_last_words(TrigramsSorted[word %like% paste0("^",words3gram)][1]$word)
if (is.na(PredictedTrigram) == FALSE){
return(PredictedTrigram)
} else if(is.na(PredictedTrigram) == TRUE) {
words2gram <- paste(words[length(words)], collapse = " ") # select last word
# Output the most frequent Bigram predicted word
PredictedBigram <- stri_extract_last_words(BigramsSorted[word %like% paste0("^",words2gram)][1]$word)
if (is.na(PredictedBigram) == FALSE){
return(PredictedBigram)
} else {
return(UnigramsSorted)[1]   # return the most frequent Unigram
}
}
}
} else {
return(UnigramsSorted)[1]
}
}
nextWordPred("the")
nextWordPred("Talking to your mom has the same effect as a hug and helps reduce your")
nextWordPred("When you were in Holland you were like 1 inch away from me but you hadn't time to take a")
nextWordPred("I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the")
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
shiny::runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
nextWordPred("the")
nextWordPred <- function(sentence){
# clean the input sentence
words <- tolower(sentence)
words <- gsub("[[:punct:]]","", words ) #remove punctuation
words <- gsub("\\d", "", words)  #reomve numbers
words <- str_trim(words) #remove unnecessary spaces
words <- unlist(strsplit(words, split = " ")) # split the sentences into words
if(length(words)>1){
words4gram <- paste(words[(length(words)-2):length(words)], collapse = " ") # select last 3 words
# Output the most frequent 4 gram predicted word
Predicted4gram <- stri_extract_last_words(FourgramsSorted[word %like% paste0("^",words4gram)][1]$word)
if(is.na(Predicted4gram) == FALSE){
return(Predicted4gram)
} else if(is.na(Predicted4gram) == TRUE) {
words3gram <- paste(words[(length(words)-1):length(words)], collapse = " ") # select last 2 words
# Output the most frequent Trigram predicted word
PredictedTrigram <- stri_extract_last_words(TrigramsSorted[word %like% paste0("^",words3gram)][1]$word)
if (is.na(PredictedTrigram) == FALSE){
return(PredictedTrigram)
} else if(is.na(PredictedTrigram) == TRUE) {
words2gram <- paste(words[length(words)], collapse = " ") # select last word
# Output the most frequent Bigram predicted word
PredictedBigram <- stri_extract_last_words(BigramsSorted[word %like% paste0("^",words2gram)][1]$word)
if (is.na(PredictedBigram) == FALSE){
return(PredictedBigram)
} else {
return(UnigramsSorted)[1]   # return the most frequent Unigram
}
}
}
} else {
return(UnigramsSorted)[1]
}
}
nextWordPred("the")
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
nextWordPred("the")
shiny::runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
nextWordPred <- function(sentence){
# clean the input sentence
words <- tolower(sentence)
words <- gsub("[[:punct:]]","", words ) #remove punctuation
words <- gsub("\\d", "", words)  #reomve numbers
words <- str_trim(words) #remove unnecessary spaces
words <- unlist(strsplit(words, split = " ")) # split the sentences into words
if(length(words)>1){
words4gram <- paste(words[(length(words)-2):length(words)], collapse = " ") # select last 3 words
# Output the most frequent 4 gram predicted word
Predicted4gram <- stri_extract_last_words(FourgramsSorted[word %like% paste0("^",words4gram)][1]$word)
if(is.na(Predicted4gram) == FALSE){
return(Predicted4gram)
} else if(is.na(Predicted4gram) == TRUE) {
words3gram <- paste(words[(length(words)-1):length(words)], collapse = " ") # select last 2 words
# Output the most frequent Trigram predicted word
PredictedTrigram <- stri_extract_last_words(TrigramsSorted[word %like% paste0("^",words3gram)][1]$word)
if (is.na(PredictedTrigram) == FALSE){
return(PredictedTrigram)
} else if(is.na(PredictedTrigram) == TRUE) {
words2gram <- paste(words[length(words)], collapse = " ") # select last word
# Output the most frequent Bigram predicted word
PredictedBigram <- stri_extract_last_words(BigramsSorted[word %like% paste0("^",words2gram)][1]$word)
if (is.na(PredictedBigram) == FALSE){
return(PredictedBigram)
} else {
return(UnigramsSorted)[1]   # return the most frequent Unigram
}
}
}
} else {
return(UnigramsSorted)[1]
}
}
nextWordPred("the")
nextWordPred("the end")
UnigramsSorted
UnigramsSorted[1]
UnigramsSorted[1]$word
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
shiny::runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
library(rsconnect)
rsconnect::setAccountInfo(name='elbaza286', token='BDD17F39F853B4848C04E552868501FC', secret='OOmBBkiw5YlsleGBOxnFbrB4wQWp4TVfIrF3/sVn')
runApp('E:/Data science/JHU Coursera/Course 10 Capstone/DSCourseraCapstone')
